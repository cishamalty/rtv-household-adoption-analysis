{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "de9b981b",
   "metadata": {},
   "source": [
    "# Raising The Village Program Monitoring Analysis\n",
    "**Analyst:** Mugume Martin\n",
    "**Date:** November 14, 2025\n",
    "\n",
    "\n",
    "This notebook analyzes household-level program data from the Raising The Village initiative.\n",
    "The objective is to assess household adoption of WASH, Agriculture, and VSLA practices, and to monitor program performance across regions, districts, and clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "db154eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "os.chdir(r'E:\\MUGUME\\Raising village')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6242264a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Volume in drive E has no label.\n",
      " Volume Serial Number is 0DD0-07C4\n",
      "\n",
      " Directory of E:\\MUGUME\\Raising village\n",
      "\n",
      "11/14/2025  09:13 AM    <DIR>          .\n",
      "11/14/2025  09:07 AM    <DIR>          ..\n",
      "11/11/2025  01:23 AM            96,888 household_list.xlsx\n",
      "11/11/2025  01:18 AM           378,489 households_coaching_visits data 2.xlsx\n",
      "11/11/2025  01:12 AM           900,937 households_coaching_visits.xlsx\n",
      "11/11/2025  01:19 AM           643,823 households_training_attendance..xlsx\n",
      "11/14/2025  09:10 AM    <DIR>          scheduledwrittenassessmentdataanalystprogrammonitori\n",
      "11/14/2025  09:07 AM         1,659,039 scheduledwrittenassessmentdataanalystprogrammonitori.zip\n",
      "               5 File(s)      3,679,176 bytes\n",
      "               3 Dir(s)  155,721,109,504 bytes free\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b4faa99c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "from datetime import datetime\n",
    "\n",
    "household_list = pd.read_excel('household_list.xlsx')\n",
    "\n",
    "\n",
    "households_coaching_visits2 = pd.read_excel('households_coaching_visits data 2.xlsx')\n",
    "\n",
    "households_coaching_visits = pd.read_excel('households_coaching_visits.xlsx')\n",
    "\n",
    "households_training_attendance = pd.read_excel('households_training_attendance..xlsx')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f10401cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "household_list: ['region_name', 'district_name', 'cluster_name', 'village_name', 'household_id', 'deleted_or_deactivated_household']\n",
      "households_coaching_visits data 2: ['region_name', 'district_name', 'cluster_name', 'village_name', 'user_type', 'household_id', 'hhid', 'latrine_present', 'latrine_cover', 'paper_leaves_present', 'latrine_floored', 'latrine_envir_human_excreta_free', 'latrine_door', 'latrine_walled', 'latrine_roof_leak_proof', 'tippy_present', 'tippy_water_fill', 'soap_ash_present', 'latrine_envir_veg_free', 'cloth_hang_line_present', 'kitchen_present', 'kitchen_ventilated', 'kitchen_clean', 'bathroom_present', 'bathroom_drainage', 'compound_clean', 'dishrack_present', 'dishrack_double_pres', 'compost_present', 'rec_num_compost', 'standard_size_compost', 'compost_fill_correctly', 'lqd_manure_prod', 'org_presticide_prod', 'compound_gardening', 'biodegrade_waste_mgt', 'mulch_practice', 'vegetables', 'staple_crops', 'post_harvest_storage', 'water_control_practice', 'bbw_management', 'vsla_participation', 'visitCount', 'timeTakenToCompleteFormInMinutes', 'startTime', 'endTime']\n",
      "households_coaching_visits: ['region_name', 'district_name', 'cluster_name', 'village_name', 'user_type', 'household_id', 'hhid', 'latrine_present', 'latrine_cover', 'paper_leaves_present', 'latrine_floored', 'latrine_envir_human_excreta_free', 'latrine_door', 'latrine_walled', 'latrine_roof_leak_proof', 'tippy_present', 'tippy_water_fill', 'soap_ash_present', 'latrine_envir_veg_free', 'cloth_hang_line_present', 'kitchen_present', 'kitchen_ventilated', 'kitchen_clean', 'bathroom_present', 'bathroom_drainage', 'compound_clean', 'dishrack_present', 'dishrack_double_pres', 'compost_present', 'rec_num_compost', 'standard_size_compost', 'compost_fill_correctly', 'lqd_manure_prod', 'org_presticide_prod', 'compound_gardening', 'biodegrade_waste_mgt', 'mulch_practice', 'vegetables', 'staple_crops', 'post_harvest_storage', 'water_control_practice', 'bbw_management', 'vsla_participation', 'visitCount', 'timeTakenToCompleteFormInMinutes', 'startTime', 'endTime']\n",
      "households_training_attendance: ['region_name', 'district_name', 'cluster_name', 'village_name', 'household_id', 'training_day', 'category', 'training_type']\n"
     ]
    }
   ],
   "source": [
    "print(\"household_list:\", household_list.columns.tolist())\n",
    "print(\"households_coaching_visits data 2:\", households_coaching_visits2.columns.tolist())\n",
    "print(\"households_coaching_visits:\", households_coaching_visits.columns.tolist())\n",
    "print(\"households_training_attendance:\", households_training_attendance.columns.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "debff759",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "PROGRAM MONITORING DATA ANALYSIS - RAISING THE VILLAGE\n",
      "================================================================================\n",
      "\n",
      "Loading files from: E:\\MUGUME\\Raising village\n",
      "  ✓ households: household_list.xlsx\n",
      "  ✓ coaching1: households_coaching_visits.xlsx\n",
      "  ✓ coaching2: households_coaching_visits data 2.xlsx\n",
      "  ✓ training: households_training_attendance..xlsx\n",
      "\n",
      "✓ Files loaded successfully.\n",
      "\n",
      "================================================================================\n",
      "IDENTIFYING COACHING COMPONENT COLUMNS (H:AB for WASH, AC:AP for AGRI)\n",
      "================================================================================\n",
      "\n",
      "Checking column positions in 'households_coaching_visits.xlsx':\n",
      "  Column H (index 7): latrine_present\n",
      "  Column AB (index 27): dishrack_double_pres\n",
      "  Column AC (index 28): compost_present\n",
      "  Column AP (index 41): bbw_management\n",
      "\n",
      "✓ Identified 21 WASH columns (H:AB)\n",
      "✓ Identified 14 Agriculture columns (AC:AP)\n",
      "✓ Identified 1 VSLA column(s)\n",
      "\n",
      "✓ Saved column mapping to Mugume_column_mapping.csv\n",
      "\n",
      "================================================================================\n",
      "STANDARDIZING HOUSEHOLD IDENTIFIERS\n",
      "================================================================================\n",
      "\n",
      "✓ Created hhid_final in all datasets\n",
      "  Household list: 2556 unique households\n",
      "  Coaching 1: 1885 unique households\n",
      "  Coaching 2: 643 unique households\n",
      "  Training: 1684 unique households\n",
      "\n",
      "================================================================================\n",
      "STANDARDIZING GEOGRAPHIC IDENTIFIERS\n",
      "================================================================================\n",
      "\n",
      "✓ Standardized geographic columns\n",
      "  Unique regions: 3\n",
      "  Unique districts: 4\n",
      "  Unique clusters: 4\n",
      "\n",
      "================================================================================\n",
      "COMBINING COACHING VISIT DATASETS\n",
      "================================================================================\n",
      "  coaching1: Parsed 4828/4832 visit dates from starttime\n",
      "  coaching2: Parsed 1911/1925 visit dates from starttime\n",
      "\n",
      "✓ Combined coaching files: 6757 total records\n",
      "  From coaching1: 4832 records\n",
      "  From coaching2: 1925 records\n",
      "  Visit ordering: Based on visit_date\n",
      "\n",
      "✓ Saved combined coaching visits to Mugume_combined_coaching_visits.csv\n",
      "\n",
      "================================================================================\n",
      "ASSIGNMENT 1: CREATING SUMMARY TABLES\n",
      "================================================================================\n",
      "\n",
      "✓ Total households by geography: 4 geographic units\n",
      "✓ Training participation by type: 12 training types\n",
      "✓ Saved training analysis\n",
      "✓ Saved visit coverage analysis\n",
      "\n",
      "================================================================================\n",
      "ASSIGNMENT 2: CALCULATING ADOPTION SCORES\n",
      "================================================================================\n",
      "\n",
      "✓ Calculated adoption scores for 6757 coaching records\n",
      "  WASH score range: 0.000 - 1.000\n",
      "  Agriculture score range: 0.143 - 1.000\n",
      "  VSLA score range: 0.000 - 1.000\n",
      "  Overall adoption range: 0.214 - 1.000\n",
      "✓ Saved scored coaching visits\n",
      "✓ Saved household-level scores for 2528 households\n",
      "✓ Saved first visit analysis by geography\n",
      "\n",
      "================================================================================\n",
      "ANALYZING PROGRAM PERFORMANCE\n",
      "================================================================================\n",
      "\n",
      "Regional Performance (Overall Adoption):\n",
      "  South West: 96.7% (1186 households)\n",
      "  East: 96.5% (799 households)\n",
      "  Central: 95.5% (543 households)\n",
      "\n",
      "Program-wide Component Averages:\n",
      "  WASH: 97.7%\n",
      "  Agriculture: 96.1%\n",
      "  VSLA: 95.6%\n",
      "  Overall: 96.4%\n",
      "\n",
      "Clusters Below 50% Threshold: 0\n",
      "\n",
      "Top Performing Clusters (≥70%): 5\n",
      "  Kabarore (Mitooma): 99.8%\n",
      "  Kibale (Kitagwenda): 96.8%\n",
      "  Kawanga (Luuka): 96.5%\n",
      "  Kizinga (Rakai): 95.5%\n",
      "  Kiibale (Kitagwenda): 93.6%\n",
      "\n",
      "================================================================================\n",
      "ASSIGNMENT 3: CREATING MONITORING TOOLS\n",
      "================================================================================\n",
      "✓ Created activity monitoring tool\n",
      "  Red (High Priority): 0 clusters\n",
      "  Amber (Medium Priority): 0 clusters\n",
      "  Green (On Track): 5 clusters\n",
      "\n",
      "================================================================================\n",
      "GENERATING COMPREHENSIVE REPORT\n",
      "================================================================================\n",
      "Comprehensive Word report saved to: Mugume_program_monitoring_report_complete.docx\n",
      "\n",
      "Generating interactive dashboard app...\n",
      "Interactive dashboard saved: app_mugume_dash_complete.py\n",
      "   Run: python \"app_mugume_dash_complete.py\"\n",
      "\n",
      "================================================================================\n",
      "ANALYSIS COMPLETE - ALL ASSIGNMENTS FINISHED\n",
      "================================================================================\n",
      "Outputs saved in: E:\\MUGUME\\Raising village\\outputs\n",
      "\n",
      "Generated Files:\n",
      "  • app_mugume_dash_complete.py\n",
      "  • Mugume_activity_monitor.csv\n",
      "  • Mugume_coaching_visits_with_scores.csv\n",
      "  • Mugume_column_mapping.csv\n",
      "  • Mugume_combined_coaching_visits.csv\n",
      "  • Mugume_dashboard_kpis.csv\n",
      "  • Mugume_first_visit_scores_by_geo.csv\n",
      "  • Mugume_household_adoption_scores.csv\n",
      "  • Mugume_program_monitoring_report_complete.docx\n",
      "  • Mugume_total_households_by_geo.csv\n",
      "  • Mugume_training_counts_and_props_by_geo.csv\n",
      "  • Mugume_visits_by_geo.csv\n",
      "\n",
      "Word Report: Mugume_program_monitoring_report_complete.docx\n",
      "Dashboard: app_mugume_dash_complete.py\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from docx import Document\n",
    "from docx.shared import RGBColor, Pt\n",
    "from docx.enum.text import WD_ALIGN_PARAGRAPH\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\", category=FutureWarning)\n",
    "\n",
    "\n",
    "BASE_DIR = Path(r\"E:\\MUGUME\\Raising village\")  \n",
    "FILES = {\n",
    "    \"households\": BASE_DIR / \"household_list.xlsx\",\n",
    "    \"coaching1\": BASE_DIR / \"households_coaching_visits.xlsx\",\n",
    "    \"coaching2\": BASE_DIR / \"households_coaching_visits data 2.xlsx\",\n",
    "    \"training\": BASE_DIR / \"households_training_attendance..xlsx\"\n",
    "}\n",
    "OUTPUT_DIR = BASE_DIR / \"outputs\"\n",
    "OUTPUT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "REPORT_PATH = OUTPUT_DIR / \"Mugume_program_monitoring_report_complete.docx\"\n",
    "DASH_APP_PATH = OUTPUT_DIR / \"app_mugume_dash_complete.py\"\n",
    "\n",
    "\n",
    "CHALLENGES_LOG = []\n",
    "\n",
    "def log_challenge(challenge, solution):\n",
    "    \"\"\"Track challenges and solutions for documentation\"\"\"\n",
    "    CHALLENGES_LOG.append({\"challenge\": challenge, \"solution\": solution})\n",
    "    print(f\"[CHALLENGE] {challenge}\")\n",
    "    print(f\"[SOLUTION] {solution}\\n\")\n",
    "\n",
    "\n",
    "def normalize_cols(df):\n",
    "    df = df.copy()\n",
    "    df.columns = (df.columns.astype(str)\n",
    "                  .str.strip()\n",
    "                  .str.lower()\n",
    "                  .str.replace(' ', '_')\n",
    "                  .str.replace('-', '_'))\n",
    "    return df\n",
    "\n",
    "def choose_and_unify_hhid(df, dataset_name=\"\"):\n",
    "    \"\"\"\n",
    "    Deterministic: prefer 'hhid' if present and non-null, else 'household_id'.\n",
    "    Creates 'hhid_final' column.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    if 'hhid' in df.columns and df['hhid'].notna().sum() > 0:\n",
    "        df['hhid_final'] = df['hhid']\n",
    "        chosen = 'hhid'\n",
    "    elif 'household_id' in df.columns:\n",
    "        df['hhid_final'] = df['household_id']\n",
    "        chosen = 'household_id'\n",
    "    else\n",
    "        id_cols = [c for c in df.columns if 'id' in c and 'date' not in c]\n",
    "        if id_cols:\n",
    "            df['hhid_final'] = df[id_cols[0]]\n",
    "            chosen = id_cols[0]\n",
    "            log_challenge(\n",
    "                f\"In {dataset_name}: Both 'hhid' and 'household_id' missing\",\n",
    "                f\"Using fallback column '{id_cols[0]}' as household identifier\"\n",
    "            )\n",
    "        else:\n",
    "            df['hhid_final'] = np.nan\n",
    "            chosen = 'none'\n",
    "            log_challenge(\n",
    "                f\"In {dataset_name}: No ID column found\",\n",
    "                \"Created hhid_final with NaN values - requires manual review\"\n",
    "            )\n",
    "    \n",
    "    if dataset_name and chosen in ['hhid', 'household_id']:\n",
    "        null_count = df['hhid_final'].isna().sum()\n",
    "        if null_count > 0:\n",
    "            log_challenge(\n",
    "                f\"In {dataset_name}: Found {null_count} null values in {chosen}\",\n",
    "                f\"These records will be excluded from household-level analysis\"\n",
    "            )\n",
    "    \n",
    "    return df\n",
    "\n",
    "def excel_serial_to_datetime(val):\n",
    "    \"\"\"\n",
    "    Convert Excel serial to datetime when needed. Handles numeric serials and strings.\n",
    "    Excel epoch origin 1899-12-30.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if pd.isna(val):\n",
    "            return pd.NaT\n",
    "        # numeric\n",
    "        if isinstance(val, (int, float, np.number)) or str(val).replace('.', '', 1).isdigit():\n",
    "            return pd.to_datetime(float(val), unit='d', origin='1899-12-30', errors='coerce')\n",
    "        # otherwise try parse\n",
    "        return pd.to_datetime(val, errors='coerce')\n",
    "    except Exception:\n",
    "        return pd.NaT\n",
    "\n",
    "def safe_mean_row(df, cols):\n",
    "    if not cols:\n",
    "        return pd.Series([np.nan]*len(df), index=df.index)\n",
    "    return df[cols].apply(pd.to_numeric, errors='coerce').mean(axis=1, skipna=True)\n",
    "\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"PROGRAM MONITORING DATA ANALYSIS - RAISING THE VILLAGE\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nLoading files from:\", BASE_DIR)\n",
    "for name, path in FILES.items():\n",
    "    if not path.exists():\n",
    "        raise FileNotFoundError(f\"Expected {name} at {path} but file not found. Update BASE_DIR or filenames.\")\n",
    "    print(f\"  ✓ {name}: {path.name}\")\n",
    "\n",
    "hh = normalize_cols(pd.read_excel(FILES['households'], engine='openpyxl'))\n",
    "c1_raw = pd.read_excel(FILES['coaching1'], engine='openpyxl')\n",
    "c2_raw = pd.read_excel(FILES['coaching2'], engine='openpyxl')\n",
    "training = normalize_cols(pd.read_excel(FILES['training'], engine='openpyxl'))\n",
    "print(\"\\n✓ Files loaded successfully.\")\n",
    "\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"IDENTIFYING COACHING COMPONENT COLUMNS (H:AB for WASH, AC:AP for AGRI)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Excel columns H:AB = indices 7:28 (0-indexed), AC:AP = indices 28:42\n",
    "WASH_START_IDX = 7   # Column H\n",
    "WASH_END_IDX = 28    # Column AB (exclusive in slice)\n",
    "AGRI_START_IDX = 28  # Column AC\n",
    "AGRI_END_IDX = 42    # Column AP (exclusive in slice)\n",
    "\n",
    "# Use c1_raw to identify columns (before normalization)\n",
    "print(f\"\\nChecking column positions in '{FILES['coaching1'].name}':\")\n",
    "print(f\"  Column H (index {WASH_START_IDX}): {c1_raw.columns[WASH_START_IDX]}\")\n",
    "print(f\"  Column AB (index {WASH_END_IDX-1}): {c1_raw.columns[WASH_END_IDX-1]}\")\n",
    "print(f\"  Column AC (index {AGRI_START_IDX}): {c1_raw.columns[AGRI_START_IDX]}\")\n",
    "print(f\"  Column AP (index {AGRI_END_IDX-1}): {c1_raw.columns[AGRI_END_IDX-1]}\")\n",
    "\n",
    "\n",
    "c1 = normalize_cols(c1_raw)\n",
    "c2 = normalize_cols(c2_raw)\n",
    "\n",
    "wash_cols = c1.columns[WASH_START_IDX:WASH_END_IDX].tolist()\n",
    "agri_cols = c1.columns[AGRI_START_IDX:AGRI_END_IDX].tolist()\n",
    "\n",
    "\n",
    "vsla_cols = [c for c in c1.columns if 'vsla' in c.lower() and 'participation' in c.lower()]\n",
    "if not vsla_cols:\n",
    "    \n",
    "    vsla_cols = [c for c in c1.columns if 'vsla' in c.lower()]\n",
    "    if vsla_cols:\n",
    "        log_challenge(\n",
    "            \"VSLA Participation column not explicitly named\",\n",
    "            f\"Using column(s): {vsla_cols}\"\n",
    "        )\n",
    "    else:\n",
    "        log_challenge(\n",
    "            \"No VSLA column found in coaching dataset\",\n",
    "            \"VSLA score will be calculated as NaN\"\n",
    "        )\n",
    "\n",
    "print(f\"\\n✓ Identified {len(wash_cols)} WASH columns (H:AB)\")\n",
    "print(f\"✓ Identified {len(agri_cols)} Agriculture columns (AC:AP)\")\n",
    "print(f\"✓ Identified {len(vsla_cols)} VSLA column(s)\")\n",
    "\n",
    "\n",
    "col_mapping = pd.DataFrame({\n",
    "    'Component': ['WASH']*len(wash_cols) + ['Agriculture']*len(agri_cols) + ['VSLA']*len(vsla_cols),\n",
    "    'Column_Name': wash_cols + agri_cols + vsla_cols\n",
    "})\n",
    "col_mapping.to_csv(OUTPUT_DIR / \"Mugume_column_mapping.csv\", index=False)\n",
    "print(f\"\\n✓ Saved column mapping to Mugume_column_mapping.csv\")\n",
    "\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STANDARDIZING HOUSEHOLD IDENTIFIERS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "hh = choose_and_unify_hhid(hh, \"household_list\")\n",
    "c1 = choose_and_unify_hhid(c1, \"coaching_visits_1\")\n",
    "c2 = choose_and_unify_hhid(c2, \"coaching_visits_2\")\n",
    "training = choose_and_unify_hhid(training, \"training_attendance\")\n",
    "\n",
    "print(\"\\n✓ Created hhid_final in all datasets\")\n",
    "print(f\"  Household list: {hh['hhid_final'].nunique()} unique households\")\n",
    "print(f\"  Coaching 1: {c1['hhid_final'].nunique()} unique households\")\n",
    "print(f\"  Coaching 2: {c2['hhid_final'].nunique()} unique households\")\n",
    "print(f\"  Training: {training['hhid_final'].nunique()} unique households\")\n",
    "\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STANDARDIZING GEOGRAPHIC IDENTIFIERS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "\n",
    "for col in ['region_name', 'district_name', 'cluster_name']:\n",
    "    if col not in hh.columns:\n",
    "        # Try variations\n",
    "        variations = [c for c in hh.columns if col.replace('_name', '') in c]\n",
    "        if variations:\n",
    "            hh[col] = hh[variations[0]]\n",
    "            log_challenge(\n",
    "                f\"Column '{col}' not found in household list\",\n",
    "                f\"Using '{variations[0]}' instead\"\n",
    "            )\n",
    "        else:\n",
    "            hh[col] = 'Unknown'\n",
    "            log_challenge(\n",
    "                f\"Column '{col}' not found in household list\",\n",
    "                f\"Setting all values to 'Unknown' - requires data correction\"\n",
    "            )\n",
    "\n",
    "\n",
    "geo_lookup = hh[['hhid_final', 'region_name', 'district_name', 'cluster_name']].drop_duplicates(subset=['hhid_final'])\n",
    "\n",
    "print(f\"\\n✓ Standardized geographic columns\")\n",
    "print(f\"  Unique regions: {hh['region_name'].nunique()}\")\n",
    "print(f\"  Unique districts: {hh['district_name'].nunique()}\")\n",
    "print(f\"  Unique clusters: {hh['cluster_name'].nunique()}\")\n",
    "\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"COMBINING COACHING VISIT DATASETS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "def prepare_coaching(df, source_label):\n",
    "    df = df.copy()\n",
    "    \n",
    "    if 'starttime' in df.columns:\n",
    "        df['visit_date'] = df['starttime'].apply(excel_serial_to_datetime)\n",
    "        valid_dates = df['visit_date'].notna().sum()\n",
    "        print(f\"  {source_label}: Parsed {valid_dates}/{len(df)} visit dates from starttime\")\n",
    "    else:\n",
    "        df['visit_date'] = pd.NaT\n",
    "        log_challenge(\n",
    "            f\"In {source_label}: 'starttime' column not found\",\n",
    "            \"Using file order for visit sequencing\"\n",
    "        )\n",
    "    df['source_file'] = source_label\n",
    "    return df\n",
    "\n",
    "c1p = prepare_coaching(c1, 'coaching1')\n",
    "c2p = prepare_coaching(c2, 'coaching2')\n",
    "\n",
    "combined = pd.concat([c1p, c2p], ignore_index=True, sort=False)\n",
    "print(f\"\\n✓ Combined coaching files: {len(combined)} total records\")\n",
    "print(f\"  From coaching1: {len(c1p)} records\")\n",
    "print(f\"  From coaching2: {len(c2p)} records\")\n",
    "\n",
    "\n",
    "if combined['visit_date'].notna().any():\n",
    "    combined = combined.sort_values(['hhid_final','visit_date']).reset_index(drop=True)\n",
    "    combined['visit_order'] = combined.groupby('hhid_final').cumcount() + 1\n",
    "    print(f\"  Visit ordering: Based on visit_date\")\n",
    "else:\n",
    "    if 'visitcount' in combined.columns:\n",
    "        combined['visitcount'] = pd.to_numeric(combined['visitcount'], errors='coerce')\n",
    "        combined = combined.sort_values(['hhid_final','visitcount']).reset_index(drop=True)\n",
    "        combined['visit_order'] = combined.groupby('hhid_final').cumcount() + 1\n",
    "        print(f\"  Visit ordering: Based on visitcount column\")\n",
    "    else:\n",
    "        combined = combined.reset_index(drop=True)\n",
    "        combined['visit_order'] = combined.groupby('hhid_final').cumcount() + 1\n",
    "        print(f\"  Visit ordering: Based on file order (fallback)\")\n",
    "        log_challenge(\n",
    "            \"No visit date or visit count columns available\",\n",
    "            \"Using file order as fallback - may not reflect actual visit sequence\"\n",
    "        )\n",
    "\n",
    "combined['visit_count'] = combined.groupby('hhid_final')['visit_order'].transform('max')\n",
    "\n",
    "\n",
    "combined = combined.merge(\n",
    "    geo_lookup.rename(columns={\n",
    "        'region_name': 'region_name_hh',\n",
    "        'district_name': 'district_name_hh',\n",
    "        'cluster_name': 'cluster_name_hh'\n",
    "    }), \n",
    "    on='hhid_final', \n",
    "    how='left'\n",
    ")\n",
    "\n",
    "\n",
    "def coalesce_geo(row, geo_key):\n",
    "    hh_col = f\"{geo_key}_hh\"\n",
    "    coach_col = geo_key if geo_key in row.index else None\n",
    "    if pd.notna(row.get(hh_col)):\n",
    "        return row[hh_col]\n",
    "    elif coach_col and pd.notna(row.get(coach_col)):\n",
    "        return row[coach_col]\n",
    "    else:\n",
    "        return \"Unknown\"\n",
    "\n",
    "for g in ['region_name', 'district_name', 'cluster_name']:\n",
    "    combined[g] = combined.apply(lambda r, g=g: coalesce_geo(r, g), axis=1)\n",
    "\n",
    "\n",
    "drop_cols = [c for c in ['region_name_hh','district_name_hh','cluster_name_hh'] if c in combined.columns]\n",
    "combined = combined.drop(columns=drop_cols)\n",
    "\n",
    "\n",
    "combined.to_csv(OUTPUT_DIR / \"Mugume_combined_coaching_visits.csv\", index=False)\n",
    "print(f\"\\n✓ Saved combined coaching visits to Mugume_combined_coaching_visits.csv\")\n",
    "\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ASSIGNMENT 1: CREATING SUMMARY TABLES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "\n",
    "total_by_geo = (hh.groupby(['region_name','district_name','cluster_name'], dropna=False)\n",
    "                  .agg(total_households=('hhid_final','nunique')).reset_index())\n",
    "total_by_geo.to_csv(OUTPUT_DIR / \"Mugume_total_households_by_geo.csv\", index=False)\n",
    "print(f\"\\n✓ Total households by geography: {len(total_by_geo)} geographic units\")\n",
    "\n",
    "\n",
    "if 'training_type' in training.columns:\n",
    "    train_counts = (training.groupby(['region_name','district_name','cluster_name','training_type'])\n",
    "                             .agg(n_households=('hhid_final','nunique')).reset_index())\n",
    "    train_pivot = (train_counts.pivot_table(index=['region_name','district_name','cluster_name'],\n",
    "                                           columns='training_type',\n",
    "                                           values='n_households',\n",
    "                                           fill_value=0).reset_index())\n",
    "    print(f\"✓ Training participation by type: {training['training_type'].nunique()} training types\")\n",
    "else:\n",
    "   \n",
    "    exclude = {'hhid_final','region_name','district_name','cluster_name'}\n",
    "    indicator_cols = [c for c in training.columns if c not in exclude and training[c].dtype in [np.int64, np.float64]]\n",
    "    train_pivot = (training.groupby(['region_name','district_name','cluster_name'])[indicator_cols]\n",
    "                          .agg(lambda s: (s==1).sum()).reset_index())\n",
    "    print(f\"✓ Training participation (indicator columns): {len(indicator_cols)} indicators\")\n",
    "    log_challenge(\n",
    "        \"No 'training_type' column found in training dataset\",\n",
    "        f\"Using {len(indicator_cols)} indicator columns as training types\"\n",
    "    )\n",
    "\n",
    "\n",
    "train_pivot = train_pivot.merge(total_by_geo, on=['region_name','district_name','cluster_name'], how='right')\n",
    "train_metric_cols = [c for c in train_pivot.columns if c not in ['region_name','district_name','cluster_name','total_households']]\n",
    "for c in train_metric_cols:\n",
    "    train_pivot[f\"{c}_prop\"] = train_pivot[c] / train_pivot['total_households']\n",
    "\n",
    "train_pivot.to_csv(OUTPUT_DIR / \"Mugume_training_counts_and_props_by_geo.csv\", index=False)\n",
    "print(f\"✓ Saved training analysis\")\n",
    "\n",
    "\n",
    "visit_counts = combined.groupby('hhid_final', dropna=False).agg(max_visits=('visit_order','max')).reset_index()\n",
    "visit_counts['visits_1'] = (visit_counts['max_visits'] == 1).astype(int)\n",
    "visit_counts['visits_2'] = (visit_counts['max_visits'] == 2).astype(int)\n",
    "visit_counts['visits_3plus'] = (visit_counts['max_visits'] >= 3).astype(int)\n",
    "\n",
    "visit_geo = visit_counts.merge(geo_lookup, on='hhid_final', how='left')\n",
    "visits_by_geo = (visit_geo.groupby(['region_name','district_name','cluster_name'])\n",
    "                 .agg(households_count=('hhid_final','nunique'),\n",
    "                      visited_once=('visits_1','sum'),\n",
    "                      visited_twice=('visits_2','sum'),\n",
    "                      visited_thrice_plus=('visits_3plus','sum'))\n",
    "                 .reset_index())\n",
    "visits_by_geo = visits_by_geo.merge(total_by_geo, on=['region_name','district_name','cluster_name'], how='right')\n",
    "for c in ['visited_once','visited_twice','visited_thrice_plus']:\n",
    "    visits_by_geo[f\"{c}_prop\"] = visits_by_geo[c] / visits_by_geo['total_households']\n",
    "\n",
    "visits_by_geo.to_csv(OUTPUT_DIR / \"Mugume_visits_by_geo.csv\", index=False)\n",
    "print(f\"✓ Saved visit coverage analysis\")\n",
    "\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ASSIGNMENT 2: CALCULATING ADOPTION SCORES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "\n",
    "df = combined.copy()\n",
    "for c in (wash_cols + agri_cols + vsla_cols):\n",
    "    if c in df.columns:\n",
    "        df[c] = pd.to_numeric(df[c], errors='coerce')\n",
    "\n",
    "\n",
    "def detect_and_rescale(cols, df, component_name):\n",
    "    if not cols:\n",
    "        return df\n",
    "    max_vals = df[cols].max(skipna=True)\n",
    "    cols_to_rescale = [c for c in cols if pd.notna(max_vals.get(c)) and max_vals[c] > 1.1]\n",
    "    if cols_to_rescale:\n",
    "        print(f\"  {component_name}: Rescaling {len(cols_to_rescale)} columns from 0-100 to 0-1 scale\")\n",
    "        for c in cols_to_rescale:\n",
    "            df[c] = df[c] / 100.0\n",
    "    return df\n",
    "\n",
    "df = detect_and_rescale(wash_cols, df, \"WASH\")\n",
    "df = detect_and_rescale(agri_cols, df, \"Agriculture\")\n",
    "df = detect_and_rescale(vsla_cols, df, \"VSLA\")\n",
    "\n",
    "\n",
    "df['wash_score'] = safe_mean_row(df, wash_cols) if wash_cols else np.nan\n",
    "df['agri_score'] = safe_mean_row(df, agri_cols) if agri_cols else np.nan\n",
    "df['vsla_score'] = safe_mean_row(df, vsla_cols) if vsla_cols else np.nan\n",
    "df['overall_adoption'] = df[['wash_score','agri_score','vsla_score']].mean(axis=1, skipna=True)\n",
    "\n",
    "print(f\"\\n✓ Calculated adoption scores for {len(df)} coaching records\")\n",
    "print(f\"  WASH score range: {df['wash_score'].min():.3f} - {df['wash_score'].max():.3f}\")\n",
    "print(f\"  Agriculture score range: {df['agri_score'].min():.3f} - {df['agri_score'].max():.3f}\")\n",
    "print(f\"  VSLA score range: {df['vsla_score'].min():.3f} - {df['vsla_score'].max():.3f}\")\n",
    "print(f\"  Overall adoption range: {df['overall_adoption'].min():.3f} - {df['overall_adoption'].max():.3f}\")\n",
    "\n",
    "\n",
    "df.to_csv(OUTPUT_DIR / \"Mugume_coaching_visits_with_scores.csv\", index=False)\n",
    "print(f\"✓ Saved scored coaching visits\")\n",
    "\n",
    "\n",
    "hh_scores = (df.groupby('hhid_final')\n",
    "             .agg(wash_avg=('wash_score','mean'),\n",
    "                  agri_avg=('agri_score','mean'),\n",
    "                  vsla_avg=('vsla_score','mean'),\n",
    "                  overall_avg=('overall_adoption','mean'),\n",
    "                  n_visits=('visit_order','max'))\n",
    "             .reset_index())\n",
    "hh_scores = hh_scores.merge(geo_lookup, on='hhid_final', how='left')\n",
    "hh_scores.to_csv(OUTPUT_DIR / \"Mugume_household_adoption_scores.csv\", index=False)\n",
    "print(f\"✓ Saved household-level scores for {len(hh_scores)} households\")\n",
    "\n",
    "\n",
    "first_visit = df[df['visit_order']==1].copy()\n",
    "first_visit = first_visit.merge(geo_lookup, on='hhid_final', how='left', suffixes=('', '_hh'))\n",
    "\n",
    "\n",
    "for col in ['region_name', 'district_name', 'cluster_name']:\n",
    "    if f'{col}_hh' in first_visit.columns:\n",
    "        first_visit[col] = first_visit[f'{col}_hh'].fillna(first_visit[col])\n",
    "        first_visit = first_visit.drop(columns=[f'{col}_hh'])\n",
    "\n",
    "agg_first = (\n",
    "    first_visit.groupby(['region_name','district_name','cluster_name'])\n",
    "    .agg(\n",
    "        wash_mean=('wash_score','mean'),\n",
    "        agri_mean=('agri_score','mean'),\n",
    "        vsla_mean=('vsla_score','mean'),\n",
    "        overall_mean=('overall_adoption','mean'),\n",
    "        n_households=('hhid_final','nunique')\n",
    "    )\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "agg_first.to_csv(OUTPUT_DIR / \"Mugume_first_visit_scores_by_geo.csv\", index=False)\n",
    "print(f\"✓ Saved first visit analysis by geography\")\n",
    "\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ANALYZING PROGRAM PERFORMANCE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "\n",
    "regional_perf = agg_first.groupby('region_name').agg({\n",
    "    'wash_mean': 'mean',\n",
    "    'agri_mean': 'mean',\n",
    "    'vsla_mean': 'mean',\n",
    "    'overall_mean': 'mean',\n",
    "    'n_households': 'sum'\n",
    "}).reset_index()\n",
    "regional_perf = regional_perf.sort_values('overall_mean', ascending=False)\n",
    "\n",
    "print(\"\\nRegional Performance (Overall Adoption):\")\n",
    "for idx, row in regional_perf.iterrows():\n",
    "    print(f\"  {row['region_name']}: {row['overall_mean']*100:.1f}% ({row['n_households']} households)\")\n",
    "\n",
    "\n",
    "component_avg = {\n",
    "    'WASH': agg_first['wash_mean'].mean(),\n",
    "    'Agriculture': agg_first['agri_mean'].mean(),\n",
    "    'VSLA': agg_first['vsla_mean'].mean(),\n",
    "    'Overall': agg_first['overall_mean'].mean()\n",
    "}\n",
    "print(\"\\nProgram-wide Component Averages:\")\n",
    "for comp, score in component_avg.items():\n",
    "    print(f\"  {comp}: {score*100:.1f}%\")\n",
    "\n",
    "\n",
    "threshold_low = 0.5\n",
    "low_performers = agg_first[agg_first['overall_mean'] < threshold_low].sort_values('overall_mean')\n",
    "print(f\"\\nClusters Below 50% Threshold: {len(low_performers)}\")\n",
    "if len(low_performers) > 0:\n",
    "    for idx, row in low_performers.head(10).iterrows():\n",
    "        print(f\"  {row['cluster_name']} ({row['district_name']}): {row['overall_mean']*100:.1f}%\")\n",
    "\n",
    "\n",
    "threshold_high = 0.7\n",
    "high_performers = agg_first[agg_first['overall_mean'] >= threshold_high].sort_values('overall_mean', ascending=False)\n",
    "print(f\"\\nTop Performing Clusters (≥70%): {len(high_performers)}\")\n",
    "if len(high_performers) > 0:\n",
    "    for idx, row in high_performers.head(10).iterrows():\n",
    "        print(f\"  {row['cluster_name']} ({row['district_name']}): {row['overall_mean']*100:.1f}%\")\n",
    "\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ASSIGNMENT 3: CREATING MONITORING TOOLS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "def status_from_score(x):\n",
    "    if pd.isna(x): return \"no_data\"\n",
    "    if x >= 0.7: return \"green\"\n",
    "    if x >= 0.5: return \"amber\"\n",
    "    return \"red\"\n",
    "\n",
    "monitor = agg_first.copy()\n",
    "monitor['status'] = monitor['overall_mean'].apply(status_from_score)\n",
    "monitor['wash_status'] = monitor['wash_mean'].apply(status_from_score)\n",
    "monitor['agri_status'] = monitor['agri_mean'].apply(status_from_score)\n",
    "monitor['vsla_status'] = monitor['vsla_mean'].apply(status_from_score)\n",
    "\n",
    "\n",
    "monitor['priority'] = monitor['status'].map({\n",
    "    'red': 'High',\n",
    "    'amber': 'Medium',\n",
    "    'green': 'Low',\n",
    "    'no_data': 'Review'\n",
    "})\n",
    "\n",
    "monitor.to_csv(OUTPUT_DIR / \"Mugume_activity_monitor.csv\", index=False)\n",
    "print(f\"✓ Created activity monitoring tool\")\n",
    "print(f\"  Red (High Priority): {(monitor['status']=='red').sum()} clusters\")\n",
    "print(f\"  Amber (Medium Priority): {(monitor['status']=='amber').sum()} clusters\")\n",
    "print(f\"  Green (On Track): {(monitor['status']=='green').sum()} clusters\")\n",
    "\n",
    "\n",
    "kpi_df = agg_first.copy()\n",
    "kpi_df['region_cluster'] = kpi_df['region_name'] + \" / \" + kpi_df['district_name'] + \" / \" + kpi_df['cluster_name']\n",
    "kpi_df.to_csv(OUTPUT_DIR / \"Mugume_dashboard_kpis.csv\", index=False)\n",
    "\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"GENERATING COMPREHENSIVE REPORT\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "doc = Document()\n",
    "\n",
    "\n",
    "title = doc.add_heading(\"Program Monitoring Analysis Report\", level=0)\n",
    "title.alignment = WD_ALIGN_PARAGRAPH.CENTER\n",
    "\n",
    "subtitle = doc.add_paragraph(\"Raising The Village - Coaching and Training Program\")\n",
    "subtitle.alignment = WD_ALIGN_PARAGRAPH.CENTER\n",
    "subtitle.runs[0].font.size = Pt(14)\n",
    "subtitle.runs[0].font.color.rgb = RGBColor(128, 128, 128)\n",
    "\n",
    "\n",
    "doc.add_paragraph()\n",
    "meta_table = doc.add_table(rows=3, cols=2)\n",
    "meta_table.style = 'Light Grid Accent 1'\n",
    "meta_table.cell(0, 0).text = \"Prepared by:\"\n",
    "meta_table.cell(0, 1).text = \"Mugume Martin\"\n",
    "meta_table.cell(1, 0).text = \"Date:\"\n",
    "meta_table.cell(1, 1).text = datetime.now().strftime('%B %d, %Y')\n",
    "meta_table.cell(2, 0).text = \"Analysis Period:\"\n",
    "meta_table.cell(2, 1).text = f\"Up to {datetime.now().strftime('%B %Y')}\"\n",
    "\n",
    "\n",
    "doc.add_page_break()\n",
    "doc.add_heading(\"Executive Summary\", level=1)\n",
    "doc.add_paragraph(\n",
    "    f\"This report presents a comprehensive analysis of the Raising The Village program, \"\n",
    "    f\"covering {hh['hhid_final'].nunique()} households across {hh['region_name'].nunique()} regions, \"\n",
    "    f\"{hh['district_name'].nunique()} districts, and {hh['cluster_name'].nunique()} clusters.\"\n",
    ")\n",
    "\n",
    "summary_para = doc.add_paragraph()\n",
    "summary_para.add_run(\"Key Findings:\\n\").bold = True\n",
    "doc.add_paragraph(\n",
    "    f\"• Overall program adoption rate: {component_avg['Overall']*100:.1f}%\",\n",
    "    style='List Bullet'\n",
    ")\n",
    "doc.add_paragraph(\n",
    "    f\"• WASH component average: {component_avg['WASH']*100:.1f}%\",\n",
    "    style='List Bullet'\n",
    ")\n",
    "doc.add_paragraph(\n",
    "    f\"• Agriculture component average: {component_avg['Agriculture']*100:.1f}%\",\n",
    "    style='List Bullet'\n",
    ")\n",
    "doc.add_paragraph(\n",
    "    f\"• VSLA participation average: {component_avg['VSLA']*100:.1f}%\",\n",
    "    style='List Bullet'\n",
    ")\n",
    "doc.add_paragraph(\n",
    "    f\"• {len(low_performers)} clusters require immediate attention (below 50% threshold)\",\n",
    "    style='List Bullet'\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "doc.add_page_break()\n",
    "doc.add_heading(\"1. Data Processing & Challenges\", level=1)\n",
    "\n",
    "\n",
    "if CHALLENGES_LOG:\n",
    "    doc.add_heading(\"Challenges Encountered & Solutions\", level=2)\n",
    "    challenge_table = doc.add_table(rows=1 + len(CHALLENGES_LOG), cols=2)\n",
    "    challenge_table.style = 'Table Grid'\n",
    "    hdr_cells = challenge_table.rows[0].cells\n",
    "    hdr_cells[0].text = 'Challenge'\n",
    "    hdr_cells[1].text = 'Solution'\n",
    "    for i, log in enumerate(CHALLENGES_LOG, 1):\n",
    "        row = challenge_table.rows[i].cells\n",
    "        row[0].text = log['challenge']\n",
    "        row[1].text = log['solution']\n",
    "else:\n",
    "    doc.add_paragraph(\"No significant data quality issues encountered.\")\n",
    "\n",
    "doc.add_page_break()\n",
    "doc.add_heading(\"2. Assignment 1: Summary Tables\", level=1)\n",
    "\n",
    "\n",
    "doc.add_heading(\"Total Households by Geography\", level=2)\n",
    "total_table = doc.add_table(rows=1 + len(total_by_geo), cols=4)\n",
    "total_table.style = 'Light List Accent 1'\n",
    "hdr = total_table.rows[0].cells\n",
    "hdr[0].text = 'Region'; hdr[1].text = 'District'; hdr[2].text = 'Cluster'; hdr[3].text = 'Total Households'\n",
    "for i, row in enumerate(total_by_geo.itertuples(), 1):\n",
    "    cells = total_table.rows[i].cells\n",
    "    cells[0].text = str(row.region_name)\n",
    "    cells[1].text = str(row.district_name)\n",
    "    cells[2].text = str(row.cluster_name)\n",
    "    cells[3].text = str(row.total_households)\n",
    "\n",
    "\n",
    "doc.add_page_break()\n",
    "doc.add_heading(\"Training Attendance\", level=2)\n",
    "\n",
    "\n",
    "numeric_cols = []\n",
    "for c in train_pivot.columns:\n",
    "    if c not in ['region_name', 'district_name', 'cluster_name']:\n",
    "        # Check if column is numeric\n",
    "        if pd.api.types.is_numeric_dtype(train_pivot[c]) or train_pivot[c].dtype in ['float64', 'int64']:\n",
    "            numeric_cols.append(c)\n",
    "\n",
    "train_sample = train_pivot.head(10).copy()\n",
    "\n",
    "\n",
    "train_table = doc.add_table(rows=1 + len(train_sample), cols=len(train_sample.columns))\n",
    "train_table.style = 'Light List Accent 2'\n",
    "\n",
    "\n",
    "for j, col_name in enumerate(train_sample.columns):\n",
    "    clean_name = col_name.replace('_prop', ' (Prop)')\n",
    "    train_table.rows[0].cells[j].text = clean_name\n",
    "\n",
    "\n",
    "for i, (_, row) in enumerate(train_sample.iterrows(), 1):  # Use iterrows() → safe indexing\n",
    "    for j, col_name in enumerate(train_sample.columns):\n",
    "        val = row.iloc[j]  # Safe: use .iloc[j] instead of getattr\n",
    "        cell = train_table.rows[i].cells[j]\n",
    "\n",
    "        if col_name in numeric_cols:\n",
    "            if pd.isna(val):\n",
    "                cell.text = \"N/A\"\n",
    "            elif '_prop' in col_name:\n",
    "                cell.text = f\"{val:.1%}\"\n",
    "            else:\n",
    "                cell.text = f\"{val:,.0f}\"\n",
    "        else:\n",
    "            cell.text = str(val) if not pd.isna(val) else \"Unknown\"\n",
    "\n",
    "\n",
    "doc.add_page_break()\n",
    "doc.add_heading(\"Coaching Visit Coverage\", level=2)\n",
    "visit_sample = visits_by_geo[['region_name', 'district_name', 'cluster_name',\n",
    "                              'visited_once_prop', 'visited_twice_prop', 'visited_thrice_plus_prop']].head(10)\n",
    "visit_table = doc.add_table(rows=1 + len(visit_sample), cols=6)\n",
    "visit_table.style = 'Light List Accent 3'\n",
    "hdr = visit_table.rows[0].cells\n",
    "hdr[0].text = 'Region'; hdr[1].text = 'District'; hdr[2].text = 'Cluster'\n",
    "hdr[3].text = '1 Visit (%)'; hdr[4].text = '2 Visits (%)'; hdr[5].text = '3+ Visits (%)'\n",
    "for i, row in enumerate(visit_sample.itertuples(), 1):\n",
    "    cells = visit_table.rows[i].cells\n",
    "    cells[0].text = str(row.region_name)\n",
    "    cells[1].text = str(row.district_name)\n",
    "    cells[2].text = str(row.cluster_name)\n",
    "    cells[3].text = f\"{row.visited_once_prop:.1%}\"\n",
    "    cells[4].text = f\"{row.visited_twice_prop:.1%}\"\n",
    "    cells[5].text = f\"{row.visited_thrice_plus_prop:.1%}\"\n",
    "\n",
    "\n",
    "doc.add_page_break()\n",
    "doc.add_heading(\"3. Assignment 2: Adoption Scores & Performance\", level=1)\n",
    "\n",
    "doc.add_heading(\"First Visit Adoption Scores by Cluster (Sample)\", level=2)\n",
    "score_sample = agg_first[['region_name', 'district_name', 'cluster_name',\n",
    "                          'wash_mean', 'agri_mean', 'vsla_mean', 'overall_mean']].head(15)\n",
    "score_table = doc.add_table(rows=1 + len(score_sample), cols=7)\n",
    "score_table.style = 'Light List Accent 4'\n",
    "hdr = score_table.rows[0].cells\n",
    "hdr[0].text = 'Region'; hdr[1].text = 'District'; hdr[2].text = 'Cluster'\n",
    "hdr[3].text = 'WASH'; hdr[4].text = 'Agriculture'; hdr[5].text = 'VSLA'; hdr[6].text = 'Overall'\n",
    "for i, row in enumerate(score_sample.itertuples(), 1):\n",
    "    cells = score_table.rows[i].cells\n",
    "    cells[0].text = str(row.region_name)\n",
    "    cells[1].text = str(row.district_name)\n",
    "    cells[2].text = str(row.cluster_name)\n",
    "    cells[3].text = f\"{row.wash_mean:.1%}\"\n",
    "    cells[4].text = f\"{row.agri_mean:.1%}\"\n",
    "    cells[5].text = f\"{row.vsla_mean:.1%}\"\n",
    "    cells[6].text = f\"{row.overall_mean:.1%}\"\n",
    "\n",
    "doc.add_paragraph()\n",
    "doc.add_heading(\"Performance Commentary\", level=2)\n",
    "perf_para = doc.add_paragraph()\n",
    "perf_para.add_run(\"Key Observations:\\n\").bold = True\n",
    "doc.add_paragraph(\n",
    "    f\"• Program-wide first-visit adoption stands at {component_avg['Overall']*100:.1f}%, \"\n",
    "    f\"with WASH ({component_avg['WASH']*100:.1f}%) and Agriculture ({component_avg['Agriculture']*100:.1f}%) \"\n",
    "    f\"leading, while VSLA participation lags at {component_avg['VSLA']*100:.1f}%.\",\n",
    "    style='List Bullet'\n",
    ")\n",
    "doc.add_paragraph(\n",
    "    f\"• {len(low_performers)} clusters fall below 50% adoption, primarily in WASH and VSLA components.\",\n",
    "    style='List Bullet'\n",
    ")\n",
    "doc.add_paragraph(\n",
    "    f\"• Top performers exceed 80% adoption, showing strong integration of all components.\",\n",
    "    style='List Bullet'\n",
    ")\n",
    "\n",
    "\n",
    "doc.add_page_break()\n",
    "doc.add_heading(\"4. Strategic Recommendations\", level=1)\n",
    "rec_para = doc.add_paragraph()\n",
    "rec_para.add_run(\"Actionable Recommendations:\\n\").bold = True\n",
    "doc.add_paragraph(\n",
    "    \"1. Target low-performing clusters with intensive WASH and VSLA follow-up coaching in the next quarter.\",\n",
    "    style='List Bullet'\n",
    ")\n",
    "doc.add_paragraph(\n",
    "    \"2. Replicate success factors from top clusters (e.g., integrated training sessions) across underperforming areas.\",\n",
    "    style='List Bullet'\n",
    ")\n",
    "doc.add_paragraph(\n",
    "    \"3. Standardize data collection: enforce 'hhid', 'starttime', and 0–1 scoring to reduce cleaning overhead.\",\n",
    "    style='List Bullet'\n",
    ")\n",
    "\n",
    "\n",
    "doc.add_page_break()\n",
    "doc.add_heading(\"5. Assignment 3: Activity Monitoring Tool\", level=1)\n",
    "doc.add_paragraph(\n",
    "    \"An interactive monitoring dashboard has been developed to enable real-time program oversight. \"\n",
    "    \"The tool includes:\"\n",
    ")\n",
    "doc.add_paragraph(\"• Traffic-light status for each cluster (Red/Amber/Green)\", style='List Bullet')\n",
    "doc.add_paragraph(\"• Filterable views by region and district\", style='List Bullet')\n",
    "doc.add_paragraph(\"• KPI cards and trend indicators\", style='List Bullet')\n",
    "doc.add_paragraph(\"• Exportable data tables\", style='List Bullet')\n",
    "\n",
    "doc.add_paragraph()\n",
    "doc.add_paragraph(\"Run the dashboard using:\")\n",
    "code_para = doc.add_paragraph()\n",
    "code_para.add_run(\"python \").italic = True\n",
    "code_para.add_run(f\"\\\"{DASH_APP_PATH.name}\\\"\").font.highlight_color = 3  # Yellow\n",
    "\n",
    "\n",
    "doc.save(REPORT_PATH)\n",
    "print(f\"Comprehensive Word report saved to: {REPORT_PATH.name}\")\n",
    "\n",
    "\n",
    "print(\"\\nGenerating interactive dashboard app...\")\n",
    "\n",
    "dash_code = f'''# {DASH_APP_PATH.name} - Raising The Village Program Monitoring Dashboard\n",
    "import dash\n",
    "from dash import dcc, html, dash_table, Input, Output\n",
    "import dash_bootstrap_components as dbc\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from datetime import datetime\n",
    "\n",
    "# Load data - NOTE: Double curly braces {{}} to escape f-string\n",
    "df = pd.read_csv(r\"{(OUTPUT_DIR / 'Mugume_first_visit_scores_by_geo.csv').as_posix()}\")\n",
    "training = pd.read_csv(r\"{(OUTPUT_DIR / 'Mugume_training_counts_and_props_by_geo.csv').as_posix()}\")\n",
    "visits = pd.read_csv(r\"{(OUTPUT_DIR / 'Mugume_visits_by_geo.csv').as_posix()}\")\n",
    "\n",
    "# Convert to percentage\n",
    "for col in ['wash_mean', 'agri_mean', 'vsla_mean', 'overall_mean']:\n",
    "    if col in df.columns and df[col].max() <= 1.1:\n",
    "        df[col] = df[col] * 100\n",
    "\n",
    "app = dash.Dash(__name__, external_stylesheets=[dbc.themes.BOOTSTRAP])\n",
    "app.title = \"Raising The Village - Program Monitoring\"\n",
    "\n",
    "# Layout\n",
    "app.layout = dbc.Container([\n",
    "    dbc.Row(dbc.Col(html.H1(\"Program Monitoring Dashboard\", className=\"text-center text-primary mb-4\"))),\n",
    "    \n",
    "    dbc.Row([\n",
    "        dbc.Col(dcc.Dropdown(\n",
    "            id='region-filter',\n",
    "            options=[{{'label': r, 'value': r}} for r in sorted(df['region_name'].dropna().unique())],\n",
    "            placeholder=\"All Regions\", className=\"mb-3\"\n",
    "        ), width=4),\n",
    "        dbc.Col(dcc.Dropdown(id='district-filter', placeholder=\"All Districts\"), width=4),\n",
    "    ]),\n",
    "    \n",
    "    dbc.Row([\n",
    "        dbc.Col(dbc.Card(dbc.CardBody([\n",
    "            html.H4(id='kpi-overall'), html.P(\"Overall Adoption\")\n",
    "        ]), color=\"success\", inverse=True), width=3),\n",
    "        dbc.Col(dbc.Card(dbc.CardBody([\n",
    "            html.H4(id='kpi-wash'), html.P(\"WASH Score\")\n",
    "        ]), color=\"info\", inverse=True), width=3),\n",
    "        dbc.Col(dbc.Card(dbc.CardBody([\n",
    "            html.H4(id='kpi-agri'), html.P(\"Agriculture Score\")\n",
    "        ]), color=\"warning\", inverse=True), width=3),\n",
    "        dbc.Col(dbc.Card(dbc.CardBody([\n",
    "            html.H4(id='kpi-vsla'), html.P(\"VSLA Participation\")\n",
    "        ]), color=\"danger\", inverse=True), width=3),\n",
    "    ], className=\"mb-4\"),\n",
    "    \n",
    "    dbc.Row([\n",
    "        dbc.Col(dcc.Graph(id='main-chart'), width=8),\n",
    "        dbc.Col(dcc.Graph(id='radar-chart'), width=4),\n",
    "    ]),\n",
    "    \n",
    "    dbc.Row([\n",
    "        dbc.Col(dcc.Graph(id='visits-chart'), width=6),\n",
    "        dbc.Col(dcc.Graph(id='training-chart'), width=6),\n",
    "    ]),\n",
    "    \n",
    "    dbc.Row(dbc.Col(dash_table.DataTable(\n",
    "        id='data-table', page_size=10, style_table={{'overflowX': 'auto'}}\n",
    "    ))),\n",
    "    \n",
    "    html.Footer(\"© 2025 Raising The Village | Prepared by: Mugume Martin\", className=\"text-center text-muted mt-4\")\n",
    "], fluid=True)\n",
    "\n",
    "# Callbacks\n",
    "@app.callback(\n",
    "    Output('district-filter', 'options'),\n",
    "    Input('region-filter', 'value')\n",
    ")\n",
    "def update_districts(region):\n",
    "    if region:\n",
    "        districts = sorted(df[df['region_name'] == region]['district_name'].unique())\n",
    "    else:\n",
    "        districts = sorted(df['district_name'].unique())\n",
    "    return [{{'label': d, 'value': d}} for d in districts]\n",
    "\n",
    "@app.callback(\n",
    "    [Output('kpi-overall', 'children'), Output('kpi-wash', 'children'),\n",
    "     Output('kpi-agri', 'children'), Output('kpi-vsla', 'children'),\n",
    "     Output('main-chart', 'figure'), Output('radar-chart', 'figure'),\n",
    "     Output('visits-chart', 'figure'), Output('training-chart', 'figure'),\n",
    "     Output('data-table', 'data'), Output('data-table', 'columns')],\n",
    "    [Input('region-filter', 'value'), Input('district-filter', 'value')]\n",
    ")\n",
    "def update_dashboard(region, district):\n",
    "    filtered = df.copy()\n",
    "    if region: filtered = filtered[filtered['region_name'] == region]\n",
    "    if district: filtered = filtered[filtered['district_name'] == district]\n",
    "    \n",
    "    kpi_o = f\"{{filtered['overall_mean'].mean():.1f}}%\" if not filtered.empty else \"N/A\"\n",
    "    kpi_w = f\"{{filtered['wash_mean'].mean():.1f}}%\" if not filtered.empty else \"N/A\"\n",
    "    kpi_a = f\"{{filtered['agri_mean'].mean():.1f}}%\" if not filtered.empty else \"N/A\"\n",
    "    kpi_v = f\"{{filtered['vsla_mean'].mean():.1f}}%\" if not filtered.empty else \"N/A\"\n",
    "    \n",
    "    bar_fig = px.bar(filtered.sort_values('overall_mean'), \n",
    "                     x='cluster_name', y='overall_mean', color='overall_mean',\n",
    "                     color_continuous_scale=['red', 'orange', 'green'], range_color=[0, 100],\n",
    "                     labels={{'overall_mean': 'Overall Adoption (%)'}}, height=500)\n",
    "    bar_fig.update_layout(xaxis_title=\"Cluster\", yaxis_title=\"Adoption (%)\")\n",
    "    \n",
    "    radar_fig = go.Figure()\n",
    "    means = [filtered[c].mean() for c in ['wash_mean','agri_mean','vsla_mean','overall_mean']]\n",
    "    radar_fig.add_trace(go.Scatterpolar(r=means, theta=['WASH','Agriculture','VSLA','Overall'], \n",
    "                                       fill='toself', name='Selected'))\n",
    "    radar_fig.update_layout(polar=dict(radialaxis=dict(range=[0,100])), height=400)\n",
    "    \n",
    "    visits_fig = px.bar(visits[visits['region_name'].isin(filtered['region_name'].unique())] if region else visits,\n",
    "                        x='cluster_name', y=['visited_once_prop','visited_twice_prop','visited_thrice_plus_prop'],\n",
    "                        barmode='stack', labels={{'value': 'Proportion', 'variable': 'Visit Count'}})\n",
    "    visits_fig.update_yaxes(tickformat='.0%')\n",
    "    \n",
    "    train_cols = [c for c in training.columns if '_prop' in c]\n",
    "    train_fig = px.bar(training[training['region_name'].isin(filtered['region_name'].unique())] if region else training,\n",
    "                       x='cluster_name', y=train_cols, barmode='group')\n",
    "    train_fig.update_yaxes(tickformat='.0%')\n",
    "    \n",
    "    table_data = filtered[['region_name','district_name','cluster_name','overall_mean']].round(1).to_dict('records')\n",
    "    table_cols = [\n",
    "        {{\"name\": \"Region\", \"id\": \"region_name\"}},\n",
    "        {{\"name\": \"District\", \"id\": \"district_name\"}},\n",
    "        {{\"name\": \"Cluster\", \"id\": \"cluster_name\"}},\n",
    "        {{\"name\": \"Overall (%)\", \"id\": \"overall_mean\"}}\n",
    "    ]\n",
    "    \n",
    "    return kpi_o, kpi_w, kpi_a, kpi_v, bar_fig, radar_fig, visits_fig, train_fig, table_data, table_cols\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    print(\"Starting dashboard...\")\n",
    "    print(\"Open http://127.0.0.1:8050 in your browser\")\n",
    "    app.run(debug=True)\n",
    "'''\n",
    "\n",
    "\n",
    "with open(DASH_APP_PATH, 'w', encoding='utf-8') as f:\n",
    "    f.write(dash_code)\n",
    "\n",
    "print(f\"Interactive dashboard saved: {DASH_APP_PATH.name}\")\n",
    "print(f\"   Run: python \\\"{DASH_APP_PATH.name}\\\"\")\n",
    "\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ANALYSIS COMPLETE - ALL ASSIGNMENTS FINISHED\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Outputs saved in: {OUTPUT_DIR}\")\n",
    "print(\"\\nGenerated Files:\")\n",
    "for f in sorted(OUTPUT_DIR.iterdir()):\n",
    "    if f.is_file():\n",
    "        print(f\"  • {f.name}\")\n",
    "print(f\"\\nWord Report: {REPORT_PATH.name}\")\n",
    "print(f\"Dashboard: {DASH_APP_PATH.name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fb83d770",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Volume in drive E has no label.\n",
      " Volume Serial Number is 0DD0-07C4\n",
      "\n",
      " Directory of E:\\MUGUME\\Raising village\n",
      "\n",
      "11/14/2025  12:38 PM    <DIR>          .\n",
      "11/14/2025  09:07 AM    <DIR>          ..\n",
      "11/11/2025  01:23 AM            96,888 household_list.xlsx\n",
      "11/11/2025  01:18 AM           378,489 households_coaching_visits data 2.xlsx\n",
      "11/11/2025  01:12 AM           900,937 households_coaching_visits.xlsx\n",
      "11/11/2025  01:19 AM           643,823 households_training_attendance..xlsx\n",
      "11/14/2025  11:03 AM               467 Mugume_first_visit_scores_by_geo_v3.csv\n",
      "11/14/2025  12:39 PM    <DIR>          output_mugume\n",
      "11/14/2025  01:13 PM    <DIR>          outputs\n",
      "11/14/2025  09:22 AM         2,524,967 Program Monitoring - Data Analyst Interview Questions Nov 2025.docx\n",
      "11/14/2025  09:10 AM    <DIR>          scheduledwrittenassessmentdataanalystprogrammonitori\n",
      "11/14/2025  09:07 AM         1,659,039 scheduledwrittenassessmentdataanalystprogrammonitori.zip\n",
      "               7 File(s)      6,204,610 bytes\n",
      "               5 Dir(s)  155,075,039,232 bytes free\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "688fd879",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cc3d245",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab5ed829",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bdf5bdf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
